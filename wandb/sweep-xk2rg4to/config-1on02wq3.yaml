wandb_version: 1

batch_size:
  value: 64
checkpoint_dir:
  value: checkpoints/TinyStories_LM
context_length:
  value: 256
d_ff:
  value: 1344
d_model:
  value: 512
device:
  value: auto
dtype:
  value: float32
eval_num:
  value: 2000
learning_rate:
  value: 0.005
log_num:
  value: 2000
lr_decay_steps_ratio:
  value: 1
lr_warmup_ratio:
  value: 0.1
max_tokens:
  value: 327680000
num_heads:
  value: 16
num_layers:
  value: 4
optimizer:
  value: adamw
save_num:
  value: 5
theta:
  value: 10000
train_data_path:
  value: data/TinyStoriesV2-GPT4-train.npy
val_data_path:
  value: data/TinyStoriesV2-GPT4-valid.npy
vocab_size:
  value: 10000
